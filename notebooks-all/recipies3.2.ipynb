{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RECIPE ON RAW DATA\n",
    "1. PUt THAT UNSTRUCTURE DATA \n",
    "2. opration to be perform \n",
    "  1. 2 csv files \n",
    "  2. join  - left join on id\n",
    "  3. create new feature \n",
    "  4. aggrigate \n",
    "  5. filter raws\n",
    "  6.rename featres\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (2.1.4)\n",
      "Collecting sqlalchemy\n",
      "  Using cached SQLAlchemy-2.0.31-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
      "Collecting psycopg2\n",
      "  Using cached psycopg2-2.9.9.tar.gz (384 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy<2,>=1.22.4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas) (1.26.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sqlalchemy) (4.12.1)\n",
      "Collecting greenlet!=0.4.17 (from sqlalchemy)\n",
      "  Using cached greenlet-3.0.3-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: six>=1.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Using cached SQLAlchemy-2.0.31-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "Using cached greenlet-3.0.3-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (616 kB)\n",
      "Building wheels for collected packages: psycopg2\n",
      "  Building wheel for psycopg2 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for psycopg2: filename=psycopg2-2.9.9-cp310-cp310-linux_x86_64.whl size=166846 sha256=006411e0f16f5d6bc742a73f9cb80ed8c3385f3e29a353fd0ba98ba4557c8963\n",
      "  Stored in directory: /home/zeus/.cache/pip/wheels/7d/75/13/da1c6d88687ae81bf5e3cfa07d702981ba137963163472b050\n",
      "Successfully built psycopg2\n",
      "Installing collected packages: psycopg2, greenlet, sqlalchemy\n",
      "Successfully installed greenlet-3.0.3 psycopg2-2.9.9 sqlalchemy-2.0.31\n"
     ]
    }
   ],
   "source": [
    "! pip install pandas sqlalchemy psycopg2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, Table, MetaData, Column, Integer, String, Float, DateTime\n",
    "from sqlalchemy.dialects.postgresql import insert\n",
    "import os\n",
    "\n",
    "def get_sqlalchemy_type(dtype):\n",
    "    \"\"\"Map pandas dtype to SQLAlchemy type.\"\"\"\n",
    "    if pd.api.types.is_integer_dtype(dtype):\n",
    "        return Integer\n",
    "    elif pd.api.types.is_float_dtype(dtype):\n",
    "        return Float\n",
    "    elif pd.api.types.is_datetime64_any_dtype(dtype):\n",
    "        return DateTime\n",
    "    else:\n",
    "        return String\n",
    "\n",
    "def create_table_from_csv(engine, table_name, csv_file):\n",
    "    \"\"\"Create a table in the database based on the CSV file structure.\"\"\"\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    metadata = MetaData()\n",
    "    columns = [\n",
    "        Column('id', Integer, primary_key=True, autoincrement=True)  # Add an id column as the primary key\n",
    "    ]\n",
    "\n",
    "    for column_name, dtype in df.dtypes.items():\n",
    "        columns.append(Column(column_name, get_sqlalchemy_type(dtype)))\n",
    "\n",
    "    table = Table(table_name, metadata, *columns)\n",
    "\n",
    "    metadata.create_all(engine)\n",
    "\n",
    "    return table\n",
    "\n",
    "def insert_data_from_csv(engine, table, csv_file):\n",
    "    \"\"\"Insert data from CSV into the database table.\"\"\"\n",
    "    df = pd.read_csv(csv_file)\n",
    "    conn = engine.connect()\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        insert_stmt = insert(table).values(row.to_dict())\n",
    "        do_update_stmt = insert_stmt.on_conflict_do_nothing(index_elements=['id'])\n",
    "        conn.execute(do_update_stmt)\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "def main():\n",
    "    # Database connection string\n",
    "    db_url = 'postgresql+psycopg2://username:password@localhost/your_database_name'\n",
    "    engine = create_engine(db_url)\n",
    "\n",
    "    # CSV file path\n",
    "    csv_file = 'path/to/your_file.csv'\n",
    "\n",
    "    # Table name\n",
    "    table_name = 'your_table_name'\n",
    "\n",
    "    # Create table and insert data\n",
    "    table = create_table_from_csv(engine, table_name, csv_file)\n",
    "    insert_data_from_csv(engine, table, csv_file)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = pd.read_csv('path/to/your_file.csv')\n",
    "data_2 = pd.read_csv('path/to/your_file.csv')\n",
    "\n",
    "\n",
    "# merge files \n",
    "\n",
    "data_merged = pd.concat([data_1, data_2], ignore_index=True)\n",
    "\n",
    "# write to csv  \n",
    "data_merged.to_csv('path/to/merged_file.csv', index=False)\n",
    "\n",
    "# create a new table in the database\n",
    "data_merged.groupby('column_name').size().reset_index(name='count').to_sql('new_table', engine, if_exists='replace', index=False)\n",
    "\n",
    "# create a new table in the database"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
